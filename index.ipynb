{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Data Cleaning\n",
    "\n",
    "## Introduction\n",
    "In this lab, we'll make use of everything we've learned about pandas, data cleaning, and Exploratory Data Analysis. In order to complete this lab, you'll have to make import, clean, combine, reshape, and visualize data to answer questions provided, as well as your own questions!\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "* Show mastery of the content covered in this section\n",
    "\n",
    "## The Dataset\n",
    "In this lab, we'll work with the comprehensive [Super Heroes Dataset](https://www.kaggle.com/claudiodavi/superhero-set/data), which can be found on Kaggle!\n",
    "\n",
    "## Goals\n",
    "* Use all available pandas knowledge to clean the dataset and deal with null values\n",
    "* Use Queries and aggregations to group the data into interesting subsets as needed\n",
    "* Use descriptive statistics and data visualization to find answers to questions we may have about the data. \n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import and alias pandas as `pd`\n",
    "* Import and alias numpy as `np`\n",
    "* Import and alias seaborn as `sns`\n",
    "* Import and alias matplotlib.pyplot as `plt`\n",
    "* Set matplotlib visualizations to display inline in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start out with recording libraries, reading in spreadsheets, then look at data\n",
    "df.isna() - gives you the same as describe, but in number format (0, etc.)\n",
    "powers_df.shape (shows you rows and columns)\n",
    "\n",
    "- you can use pd.setoptions, \"max columns\" to see all columns if you can't see the whole set \n",
    "\n",
    "what if you want to find a specific column name? Trial warping\n",
    "\n",
    "powers_df.columns() - shows you the name of call columns; can't do \"contains or is like\"....\n",
    "\n",
    "trial_str=\"Reality warping\"\n",
    "\n",
    "Reality in trial_str -->. true\n",
    "\n",
    "pd.Series(powers_df.columns).split()  --> gives you \"reality\" and \"warping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will allow you to search for a specific column name\n",
    "#apply works on a series object of panddas (that's why you need to do pd.Series)\n",
    "#apply lambda is a nice option to apply a function to every cell \n",
    "#going to the full list of columns, going one by one, lambda (x): x if \"Reality\" in column, else 0:\n",
    "#(all records who don't have \"reality\" in the value for column x would be = 0 (thrown away))\n",
    "pd.Series(powers_df.columns)[pd.Series(powers_df.columns).apply(lambda x: x if 'Reality' in x  else 0 ) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##easier way to look for null values:\n",
    "\n",
    "powers_df.isna().sum().sum() # might want to use .sum when you have too many columns to look through\n",
    ".sort_values(ascending = False) \n",
    "\n",
    "#will sort potential values for column x in ascending or descending order\n",
    "#will allow you to get a sense of most important or least important columns/values quickly\n",
    "\n",
    "powers_df.isna().sum().sort_values(ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MAKE SURE TO USE TAB to list all possible methods, attributes, etc. in your command\n",
    "##SHIFT TAB - if you put shift tab inside of () it will open up a help window for that command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "you can look at individual column counts\n",
    "heroes_df['weight'].valuecounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.apply is taking a function (you can use lambda, or create your own function as well)\n",
    "#.apply goes element wise through a series- doing operation on all\n",
    "#make x np.nan if x =-99, else x\n",
    "## np.nan is numpy null value - it shows up as nan\n",
    "one option:\n",
    "    \n",
    ".apply(lambda x: np.nan if x = -99 else x)\n",
    "\n",
    "another option: heroes_df.replace(-99.99, np.nan), inplace = True #easiest to look at solution\n",
    "#replace can work for just one series or the whole dataframe as well \n",
    "## might not want to switch everything in the dataset at one time\n",
    "\n",
    "another: heroes_df.loc[heroes_df[\"Weight\"]== - 99, \"Weight\"] = np.nan\n",
    "\n",
    "once converted to na, use .fillna to impute data for those missing values (e.g, median, mean of that value)\n",
    "#be careful with fillna (filling in your data values) because you could bias the results-\n",
    "##if you're using median/mean, you are creating more data points closer to the mean of your variable\n",
    "###could be making your regression model fit the data better than it actually is\n",
    "####important to look more at dealing with missing data later:\n",
    "######sometimes drop columns, the null values, sometimes impute date\n",
    "\n",
    "##you can consider running the model with different options for dealing with the missing data \n",
    "(e.g., dropping, imputing, etc.) to see which is the best fit?? if you're train/testing you\n",
    "only care about getting the best predictions, so might be appropriate to compare to see which\n",
    "gives you the best prediction \n",
    "\n",
    ".unique() --> gives you all unique values\n",
    ".nunique() --> number of unique values\n",
    "\n",
    "if you know exactly what you want to drop, you might use .drop (also don't use it with duplicate data!)\n",
    "#use  heroes_df[~heroes.df].deduplicated?? to.....                                                 \n",
    "\n",
    "boolean operators take precedent over everything in order of operations\n",
    "                                                                \n",
    "-coding best practices: use parentheses around conditional statements to make it read clear\n",
    "--if you have more than a few conditionals, then best to create a mask first with two conditionals\n",
    " --> essentially stepwise addition of conditional in each line of code\n",
    "\n",
    ".set_index outside the conditional bracket statements if you want to save the filter into a new variable\n",
    "                                                                \n",
    "len(set(heroes_df['name']))  \n",
    "\n",
    "sample questions:                                                                \n",
    "---- selected all rows, specific columns with .loc[]                                                           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.groupby().sum().loc[columnanme].sortvalues(){setrange].plot(kind=\"bar\")\n",
    "\n",
    "##In pandas when you are selecting columns in brackets[], you are using all indices, unlike in python\n",
    "(in python, indices start at O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, our dataset is split among two different sources--`heroes_information.csv` and `super_hero_powers.csv`.\n",
    "\n",
    "Use pandas to read in each file and store them in DataFrames in the appropriate variables below. Then, display the head of each to ensure that everything loaded correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "heroes_df = None\n",
    "powers_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as if the heroes information dataset contained an index column.  We did not specify that this dataset contained an index column, because we hadn't seen it yet. Pandas does not know how to tell apart an index column from any other data, so it stored it with the column name `Unnamed: 0`.  \n",
    "\n",
    "Our DataFrame provided row indices by default, so this column is not needed.  Drop it from the DataFrame in place in the cell below, and then display the head of `heroes_df` to ensure that it worked properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiarize Yourself With the Dataset\n",
    "\n",
    "The first step in our Exploratory Data Analysis will be to get familiar with the data.  This step includes:\n",
    "\n",
    "* Understanding the dimensionality of your dataset\n",
    "* Investigating what type of data it contains, and the data types used to store it\n",
    "* Discovering how missing values are encoded, and how many there are\n",
    "* Getting a feel for what information it does and doesn't contain\n",
    "\n",
    "In the cell below, get the descriptive statistics of each DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Null Values\n",
    "\n",
    "Starting in the cell below, detect and deal with any null values in either data frame.  Then, explain your methodology for detecting and dealing with outliers in the markdown section below.  Be sure to explain your strategy for dealing with null values in numeric columns, as well as your strategy for dealing with null values in non-numeric columns.  \n",
    "\n",
    "Note that if you need to add more cells to write code in, you can do this by:\n",
    "\n",
    "**1.** Highlighting a cell and then pressing `ESC` to enter command mode.  \n",
    "**2.** Press `A` to add a cell above the highlighted cell, or `B` to add a cell below the highlighted cell. \n",
    "\n",
    "Describe your strategy below this line:\n",
    "____________________________________________________________________________________________________________________________\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining, Grouping, and Aggregating\n",
    "\n",
    "In the cell below, join the two DataFrames.  Think about which sort of join you should use, as well as which columns you should join on.  Rename columns and manipulate as needed.  \n",
    "\n",
    "**_HINT:_** If the join throws an error message, consider setting the column you want to join on as the index for each DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join will work only if the indices match in the two dataframes\n",
    "#merge you can merge on any two columns you want to match? in any two databases -- more useable\n",
    "#concate is to combine, but literally just pushes two dataframes together (not merging the two files together)\n",
    "-->sometimes you just want to connect two dataframes because you know order doesn't matter\n",
    "\n",
    "#use inner join with merge in order to ensure you're keeping all the data for the column you're most\n",
    "interested in (if both columns have null values, you want to make sure you're not dropping all the data\n",
    "for the right variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, subset male and female heroes into different dataframes.  Create a scatterplot of the height and weight of each hero, with weight as the y-axis.  Plot both the male and female heroes subset into each dataframe, and make the color for each point in the scatterplot correspond to the gender of the superhero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Initial Investigation\n",
    "\n",
    "Next, slice the DataFrame as needed and visualize the distribution of heights and weights by gender.  You should have 4 total plots.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Slice the DataFrame into separate DataFrames by gender\n",
    "* Complete the `show_distplot` function.  This helper function should take in a DataFrame, a string containing the gender we want to visualize, and and the column name we want to visualize by gender. The function should display a distplot visualization from seaborn of the column/gender combination.  \n",
    "\n",
    "Hint: Don't forget to check the [seaborn documentation for distplot](https://seaborn.pydata.org/generated/seaborn.distplot.html) if you have questions about how to use it correctly! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_heroes_df = None\n",
    "female_heroes_df = None\n",
    "\n",
    "def show_distplot(dataframe, gender, column_name):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Male Height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Male Weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female Height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female Weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss your findings from the plots above, with respect to the distribution of height and weight by gender.  Your explanation should include discussion of any relevant summary statistics, including mean, median, mode, and the overall shape of each distribution.  \n",
    "\n",
    "Wite your answer below this line:\n",
    "____________________________________________________________________________________________________________________________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Question: Most Common Powers\n",
    "\n",
    "The rest of this notebook will be left to you to investigate the dataset by formulating your own questions, and then seeking answers using pandas and numpy.  Every answer should include some sort of visualization, when appropriate. Before moving on to formulating your own questions, use the dataset to answer the following questions about superhero powers:\n",
    "\n",
    "* What are the 5 most common powers overall?\n",
    "* What are the 5 most common powers in the Marvel Universe?\n",
    "* What are the 5 most common powers in the DC Universe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the results you found above to answer the following question:\n",
    "\n",
    "How do the top 5 powers in the Marvel and DC universes compare?  Are they similar, or are there significant differences? How do they compare to the overall trends in the entire Superheroes dataset?\n",
    "\n",
    "Wite your answer below this line:\n",
    "____________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "### Your Own Investigation\n",
    "\n",
    "For the remainder of this lab, you'll be focusing on coming up with and answering your own question, just like we did above.  Your question should not be overly simple, and should require both descriptive statistics and data visualization to answer.  In case you're unsure of what questions to ask, some sample questions have been provided below.\n",
    "\n",
    "Pick one of the following questions to investigate and answer, or come up with one of your own!\n",
    "\n",
    "* Which powers have the highest chance of co-occurring in a hero (e.g. super strength and flight), and does this differ by gender?\n",
    "* Is there a relationship between a hero's height and weight and their powerset?\n",
    "* What is the distribution of skin colors amongst alien heroes?\n",
    "\n",
    "Explain your question below this line:\n",
    "____________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "Some sample cells have been provided to give you room to work. If you need to create more cells, you can do this easily by:\n",
    "\n",
    "1. Highlighting a cell and then pressing `esc` to enter command mode.\n",
    "1. Pressing `b` to add a cell below the currently highlighted cell, or `a` to add one above it.  \n",
    "\n",
    "Be sure to include thoughtful, well-labeled visualizations to back up your analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we demonstrated our mastery of:\n",
    "* Using all of our Pandas knowledge to date to clean the dataset and deal with null values\n",
    "* Using Queries and aggregations to group the data into interesting subsets as needed\n",
    "* Using descriptive statistics and data visualization to find answers to questions we may have about the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
